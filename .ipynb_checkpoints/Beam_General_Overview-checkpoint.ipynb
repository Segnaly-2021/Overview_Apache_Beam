{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c8cb50-3b0d-46a4-a0bb-cc60d226c10a",
   "metadata": {},
   "source": [
    "# Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5306-2b93-4810-87fb-ba3239f095e9",
   "metadata": {},
   "source": [
    "This will include all you need to know to start building your own pipeline with apache beam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb762f4-6324-4d42-a472-5dd5dbe21922",
   "metadata": {},
   "source": [
    "### Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f7dbe-3232-4342-9aea-e381d3b50e50",
   "metadata": {},
   "source": [
    "Apache Beam is a multi-language framework (Python, Java, Go, ...) that helps you build and test your pipeline.\n",
    "Apache Beam is very portable, meaning that pipelines built with Beam can be run on multiple back-ends including Dataflow(Google Cloud Platform), Spark and even your Local machine (DirectRunner). In the Beam parlance, these back-ends are called *Runner*.\n",
    "\n",
    "Beam stands for *B*atch and Str*eam* which means that Apache Beam supports both Batch and Streaming processing. This is one of the greatest advantages of using Beam, the same code can be used for both batch and online processing and run on top of multiple back-ends.\n",
    "\n",
    "That being said, let's dive into components that make up a Beam pipeline:\n",
    "\n",
    "- **Pipeline:** A Pipeline encapsulates your entire data processing task, from start to finish. This includes reading input data, transforming that data, and writing output data.\n",
    "\n",
    "- **PCollection:** A PCollection represents basically the input and the output for each step in your pipeline. As Beam supports both batch and streaming, the data set can be bounded, meaning it comes from a fixed source like a file, or unbounded, meaning it comes from a continuously updating source.\n",
    "\n",
    "- **PTransform:** A PTransform represents a data processing operation, or a step, in your pipeline. Every PTransform takes one or more PCollection objects as input, performs a processing function that you provide on the elements of that PCollection, and produces zero or more output PCollection objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470a56f-9015-4533-8b6f-79c1d58a03b0",
   "metadata": {},
   "source": [
    "### Create a Pipeline\n",
    "\n",
    "To create a Beam Pipeline, you'll first have to create an instance of the Beam SDK class **Pipeline** and set some **configuration options.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5b93c-434d-427f-936e-1ba6d8a0bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    " argv = [\n",
    "        '--project={0}'.format(project),\n",
    "        '--job_name=...',\n",
    "        '--save_main_session',\n",
    "        '--staging_location=gs://{0}/...'.format(bucket),\n",
    "        '--temp_location=gs://{0}/.../temp/'.format(bucket),\n",
    "        '--setup_file=./setup.py',\n",
    "        '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "        '--max_num_workers=8',\n",
    "        '--region={}'.format(region),\n",
    "        '--runner=DataflowRunner'\n",
    "    ]\n",
    "\n",
    "with beam.Pipeline(argv=argv) as pipeline:\n",
    "  pass  # build your pipeline here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c1c92-7694-4841-96d3-f8a3605434d1",
   "metadata": {},
   "source": [
    "### Create a PCollection\n",
    "\n",
    "The most common way to create a PCollection is to read data from an external source. Each external source has an I/O adapter that has a **Read** Transform. To read data you should apply that transform to the **Pipeline object** itself.\n",
    "\n",
    "Some Characteristics of PCollection elements\n",
    "\n",
    "- They should have the same type, which can be anything.\n",
    "- They are 'immutable'.\n",
    "- They can be bounded or unbounded.\n",
    "- They are associated with a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82458cd-6bca-449e-97de-7ec33e166c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Text\n",
    "lines = pipeline | 'ReadMyFile' >> beam.io.ReadFromText(\n",
    "    'gs://some/inputData.txt')\n",
    "\n",
    "# Read from BigQuery\n",
    "flights = pipeline | 'flights:read' >> beam.io.ReadFromBigQuery(\n",
    "                    query='SELECT * FROM dsongcp.flights', use_standard_sql=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0928a-1ab0-429d-842f-6a55906ce8f9",
   "metadata": {},
   "source": [
    "### PTransforms:\n",
    "\n",
    "They require you to provide a processing logic in a form of a function (called user code) that is applied to each element of a PCollection. \n",
    "The graph bellow reminds us the immutability of PCollections - the same PCollection (Table rows)  is used as an input for two PTransforms at the same time. \n",
    "\n",
    "<img align=\"center\" src=\"./PColl_is_immutable.png\" style=\" width:500px; padding: 10px; \" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45e812-ce0d-457a-bf2a-644f028b09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[PCollection of database table rows] = [Database Table Reader] | [Read Transform]\n",
    "[PCollection of 'A' names] = [PCollection of database table rows] | [Transform A]\n",
    "[PCollection of 'B' names] = [PCollection of database table rows] | [Transform B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88476db7-2b87-48db-9fd7-096a189a63a0",
   "metadata": {},
   "source": [
    "#### Core Beam transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebebeb8-3c7b-4ae8-8d0e-56af432cf9af",
   "metadata": {},
   "source": [
    "##### **ParDo**: \n",
    "\n",
    "ParDo is the Beam transformation for generic parallel processing. It takes each element in an input PCollection, performs some processing units on that element, and emits zero, one or multiple element to an output PCollection. It's usefull for tasks such as:\n",
    "\n",
    "- **Filtering a data set**\n",
    "- **Formatting or type-converting each element in a data set.**\n",
    "- **Extracting parts of each element in a data set**: for example you might want to discard some field in your dataset and keep only those that are relevant or important for your use case.\n",
    "- **Performing computations on each element in a data set**: for example prediction on each element of your Batch set.\n",
    "\n",
    "ParDo needs a DoFn object which contains the core transformation that your data will undergo. The **process** method is the one where those transformations are coded.\n",
    "\n",
    "**Map** (One To One mapping) and **FlatMap** (One To Multiple mapping) are examples of ParDo processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8288698-3500-4f45-8578-0cac2ae03757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input PCollection of Strings.\n",
    "words = ...\n",
    "\n",
    "# The DoFn to perform on each element in the input PCollection.\n",
    "\n",
    "class ComputeWordLengthFn(beam.DoFn):\n",
    "  def process(self, element):\n",
    "    return [len(element)]\n",
    "\n",
    "\n",
    "\n",
    "# Apply a ParDo to the PCollection \"words\" to compute lengths for each word.\n",
    "word_lengths = words | beam.ParDo(ComputeWordLengthFn())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccb9c5-8d0f-4a52-9271-5f20451fa3a2",
   "metadata": {},
   "source": [
    "##### **GroupByKey**\n",
    "\n",
    "The Beam transformation for collections of Key/Value pairs. It's best suited for aggregating data that has something (the key) in common.\n",
    "When applied to a collection, GroupByKey returns a pair of variables in the following format: \n",
    "\n",
    "*Key*, *[ List of values ]*\n",
    "\n",
    "Note that if you're trying to use GroupByKey on unbounded PCollection,Beam won't like it. It will kindly throw at you an IllegalStateException error at pipeline construction time. This is because GroupByKey needs to collect all the data for a particular key before grouping them together, while unbounded PCollections are by definition not limited. It worth mentioning that GroupByKey is applied at the same level as your windowing strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad939a-cd59-4b1b-a8f7-13202a641950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input PCollection of (`string`, `int`) tuples.\n",
    "words_and_counts = ...\n",
    "\n",
    "\n",
    "grouped_words = words_and_counts | beam.GroupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a0d22-6c36-4121-b225-7de77f81a502",
   "metadata": {},
   "source": [
    "##### **CoGroupByKey**\n",
    "\n",
    "Used to join collections that have the same *Key*.\n",
    "\n",
    "*In the Beam SDK for Python, CoGroupByKey accepts a dictionary of keyed PCollections as input. As output, CoGroupByKey creates a single output PCollection that contains one key/value tuple for each key in the input PCollections. Each key’s value is a dictionary that maps each tag to an iterable of the values under they key in the corresponding PCollection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fa02-fbe2-4b23-a0f2-bb1e90382c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_list = [\n",
    "    ('amy', 'amy@example.com'),\n",
    "    ('carl', 'carl@example.com'),\n",
    "    ('julia', 'julia@example.com'),\n",
    "    ('carl', 'carl@email.com'),\n",
    "]\n",
    "phones_list = [\n",
    "    ('amy', '111-222-3333'),\n",
    "    ('james', '222-333-4444'),\n",
    "    ('amy', '333-444-5555'),\n",
    "    ('carl', '444-555-6666'),\n",
    "]\n",
    "\n",
    "emails = p | 'CreateEmails' >> beam.Create(emails_list)\n",
    "phones = p | 'CreatePhones' >> beam.Create(phones_list)\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "results = [\n",
    "    (\n",
    "        'amy',\n",
    "        {\n",
    "            'emails': ['amy@example.com'],\n",
    "            'phones': ['111-222-3333', '333-444-5555']\n",
    "        }),\n",
    "    (\n",
    "        'carl',\n",
    "        {\n",
    "            'emails': ['carl@email.com', 'carl@example.com'],\n",
    "            'phones': ['444-555-6666']\n",
    "        }),\n",
    "    ('james', {\n",
    "        'emails': [], 'phones': ['222-333-4444']\n",
    "    }),\n",
    "    ('julia', {\n",
    "        'emails': ['julia@example.com'], 'phones': []\n",
    "    }),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2918c29-944f-48dc-b8be-a6e849a821ca",
   "metadata": {},
   "source": [
    "##### **Requirements for writing user code for Beam transforms**\n",
    "\n",
    "\n",
    "In general, your user code must fulfill at least these requirements:\n",
    "\n",
    "- Your function object must be serializable.\n",
    "- Your function object must be thread-compatible, and be aware that the Beam SDKs are not thread-safe.fe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c9c6a-19c8-442b-907e-1f57a4dbacf9",
   "metadata": {},
   "source": [
    "#### Side Inputs\n",
    "\n",
    "Side inputs are additional input along with the PCollection. They are useful when a DoFn needs extra data but that data needs to be determined at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dce477-635f-47a4-848c-ad8a06648733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pipeline I/O\n",
    "\n",
    "Beam provides read and write transforms for a number of common data storage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136fa29-7632-4eae-9403-740522b32562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading \n",
    "lines = pipeline | beam.io.ReadFromText('gs://some/inputData.txt')\n",
    "\n",
    "# Writing\n",
    "output | beam.io.WriteToText('gs://some/outputData')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f988b8-e363-4667-b518-2825c5e99d58",
   "metadata": {},
   "source": [
    "#### Reading from multiple locations\n",
    "\n",
    "Reading from multiple input files requires to these files having their name following a pattern that can be captured by a glob operator.\n",
    "Note that glob operators are filesystem-specific and obey filesystem-specific consistency models.\n",
    "\n",
    "In this example, we use the glob operator (*) to read from all the input files that have a **input** prefix and a **.csv** suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31940ce8-3658-40b1-9d9b-944d6c03a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pipeline | 'ReadFromText' >> beam.io.ReadFromText(\n",
    "    'path/to/input-*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef6a91-0a53-4965-877a-cb35376deca0",
   "metadata": {},
   "source": [
    "#### Writing to multiple output files\n",
    "\n",
    "Writing to multiple output files is the default option of a write transforms in Beam. When you pass an output file name to a write transform, it is used as a prefix for the multiple output files that'll going to be created. You can also provide a suffix to each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cd6c2-1e39-4021-a916-55d96a9295ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words | 'WriteToText' >> beam.io.WriteToText(\n",
    "    '/path/to/numbers', file_name_suffix='.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f1beb-6006-404e-b970-062b7c4265b2",
   "metadata": {},
   "source": [
    "### **Windowing**\n",
    "\n",
    "Windowing subdivides a PCollection according to the timestamps of its individual elements. Each PCollection's element can belong to one or more windows according to the PCollection's windowing function, and each individual window contains a finite number of elements. Grouping transforms work then on a per-window basis. This is especially useful for unbounded data.\n",
    "\n",
    "**Note:** Beam's default windowing behavior is to gather all of your data and put it into one single window and discard all the late-arrival. Before using a grouping transform such as *GroupByKey* on an unbounded PCollection, you must do at least one of the following:\n",
    "\n",
    "- Set a non-global windowing function.\n",
    "- Set a non-default trigger.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d7902-6cd8-493a-9f15-f27fc94c748e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Windowing with bounded PCollections:**\n",
    "\n",
    "To use windowing with fixed dataset you should first assign your own timestamps to each element of that dataset. Windowing affects the way your pipeline processes data and applies Ptransforms. In the figure below, the ParDo transform gets applied multiple times per key, and once for each window.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./aggregation_windowing.png\" style=\" width:600px; padding: 10px; \">\n",
    "</div>>\r\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4acdeaa-3d6d-46e6-b77b-dff69b95a048",
   "metadata": {},
   "source": [
    "#### **Windowing Functions**\n",
    "\n",
    "Beam provides multiple windowing functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af332e99-eee5-4b6f-9efc-82b847689f24",
   "metadata": {},
   "source": [
    "- **Fixed time windows (FTW):**\n",
    "  \n",
    "FTW is the simplest windowing functions on Beam. It capture all the elements within a certain non-overlapping-time interval. Let's say, you have a 30 seconde window duration, all the elements with timestamp values from 0:00:00 up to 0:00:30 (but not included) fall into the first window, elements with timestamp from 0:00:30 up to 0:01:00 (but not included) belong to the second window, and so on.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FTW.png\" style=\" width:600px; padding: 10px; \">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3eaae5-c08d-4593-9f39-4fde65fb9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "fixed_windowed_items = (\n",
    "    items | 'window' >> beam.WindowInto(window.FixedWindows(30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad7535-46cc-40e3-afd7-e57097bfeda8",
   "metadata": {},
   "source": [
    "- **Sliding time window (STW):**\n",
    "  \n",
    "The only difference between FTW and STW is that STW can *overlap*. For example, you can have a 60 second window **duration** and a starting window every 30 second. The frequency at which a new window starts is called **period**. In the example below, you have 60 second window duration and 30 second period.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./STW.png\" style=\" width:600px; padding: 10px; \">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4fd27-787e-4663-bd2c-5cf219945343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "sliding_windowed_items = (\n",
    "    items | 'window' >> beam.WindowInto(window.SlidingWindows(60, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9656e8-c3b2-4e66-8714-49ee868b5f04",
   "metadata": {},
   "source": [
    "- **Session windows:**\n",
    "  \n",
    "The session windowind strategy is best suited for data that is irregularly distributed with respect to time. It works on a per-key basis and requires a minimal time gap above which a new window is started. Note, in the figure below, that each key has its own number of window.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./SW.png\" style=\" width:600px; padding: 10px; \">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfe9b1-b3ab-4c31-9450-92974e28880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "session_windowed_items = (\n",
    "    items | 'window' >> beam.WindowInto(window.Sessions(10 * 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca39806-5d9b-41a5-86ff-5fc7f5219f72",
   "metadata": {},
   "source": [
    "- **The single global window:**\n",
    "\n",
    "The Beam's default windowing strategy is the single global window with late data discarded. Note that this is the strategy that is used even when working with unbounded PCollections. So you should pay attention to this when applying Ptransforms such as *GroupByKey* and *Combine* which require all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ea887-8fc0-4b4f-9c22-153e94742327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "global_windowed_items = (\n",
    "    items | 'window' >> beam.WindowInto(window.GlobalWindows()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c752d5-8379-406a-917f-bf3ee7881e47",
   "metadata": {},
   "source": [
    "#### **Watermarks and late data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3045d-65b0-4af6-bbaa-eee8e624e322",
   "metadata": {},
   "source": [
    "Watermark defines what is considered as late data. Watermark is the system's notion of when all data in a certain window can be expected to have arrived in your pipeline. Any data that arrives after the watermark has passed the end of a window is considered as late data.\n",
    "\n",
    "*But why do we need watermarks?*\n",
    "\n",
    "Beam uses data timestamps to divide PCollection elements into windows. However data isn't always guaranteed to arrive in a pipeline in a time order or to always at a predictable interval. For example, let's say you have a fixed time window of 5min long, with a watermark that assume 30s of lag time between the timestamp and the time the data arrives in your pipeline (processing time). If a data record with a timestamp that belongs to the first window (from 0:00 up to 4:59) arrives at 5:38, then that record is considered as **late data.**\n",
    "\n",
    "You can allow late data by invoking the **.withAllowedLateness** operation when you set your PCollection’s windowing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34482f8-b044-474c-a233-bf0ddbf03285",
   "metadata": {},
   "outputs": [],
   "source": [
    "   pc = [Initial PCollection]\n",
    "   pc | beam.WindowInto(\n",
    "              FixedWindows(60),\n",
    "              trigger=trigger_fn,\n",
    "              accumulation_mode=accumulation_mode,\n",
    "              timestamp_combiner=timestamp_combiner,\n",
    "              allowed_lateness=Duration(seconds=2*24*60*60)) # 2 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289e113-7108-480b-b852-c4965bc3ae81",
   "metadata": {},
   "source": [
    "#### **Assigning timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94a02-3baf-41d6-86a4-0fd4fe4ae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(event_time):\n",
    "    # The isinstance function in Python is used to check if an object is an instance or subclass of a class or a tuple of classes\n",
    "    if isinstance(event_time, str):\n",
    "        # In BigQuery, this is a datetime.datetime.  In JSON, it's a string\n",
    "        # sometimes it has a T separating the date, sometimes it doesn't\n",
    "        # Handle all the possibilities\n",
    "        event_time = dt.datetime.strptime(event_time.replace('T', ' '), DATETIME_FORMAT)\n",
    "    return event_time\n",
    "\n",
    "\n",
    "def assign_timestamp(event):\n",
    "    # A timestamp is encoded information generally used in UNIX, which indicates the date and \n",
    "    # time at which a particular event has occurred. Unix time (also known as Epoch time, POSIX time, \n",
    "    # seconds since the Epoch, or UNIX Epoch time) describes a point in time. It is the number of seconds \n",
    "    # that have elapsed since the Unix epoch which is 00:00:00 UTC on 1 January 1970\n",
    "\n",
    "    try:\n",
    "        event_time = to_datetime(event['WHEELS_OFF'])\n",
    "        yield beam.window.TimestampedValue(event, event_time.timestamp())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "events = events | 'assign_time' >> beam.FlatMap(assign_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7062de8-1dcd-40cd-807f-9470a1e8d16c",
   "metadata": {},
   "source": [
    "### **Triggers**\n",
    "\n",
    "Beam uses triggers to determine when to aggregate data and attribute them to a window. When you work with Beam's default windowing configuration and default triggers, Beam outputs the aggregated result when it estimates all data has arrived, and discards all subsequent data for that window.\n",
    "\n",
    "**Note:** Triggers are different from Allowed_lateness. The later allowed data to be accepted by your pipeline while the formal allows any stage of the pipeline to process and emit result.\n",
    "\n",
    "Beam provides numbers of pre-built triggers that you can set to change this default behavior:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e524e-1cac-4c2d-a080-7b46ed02b11b",
   "metadata": {},
   "source": [
    "- **Event time trigger**\n",
    "\n",
    "The *AfterWatermark* trigger operates on event time. *AfterWatermark* fires only when the watermark passes the end of a window based on the timestamp assigned to each element.\n",
    "\n",
    "You can also configure trigger to fire before or after the end of a window. In the example below, Beam generates outputs 60s after the first data record has arrived and every time late data arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c5ff7-c481-46c4-b3c8-a0fdd3b8c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AfterWatermark(\n",
    "    early=AfterProcessingTime(delay=1 * 60), late=AfterCount(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d6b1c-b575-4102-940f-709c04d5052e",
   "metadata": {},
   "source": [
    "- **Processing time triggers**\n",
    "\n",
    "The AfterProcessingTime trigger is useful for triggering early results from a window, particularly a window with a large time frame such as a single global window. It fires after a certain amount of processing time has passed since data was received."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58c609-625b-4f54-ac1f-f45c13378668",
   "metadata": {},
   "source": [
    "- **Data-driven triggers**\n",
    "\n",
    "Beam provides one data-driven trigger, *AfterCount*. It fires after collecting a certain amount of elements. This can be particularly usefull when working with one single global window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac32c4-6cd5-460f-a028-4f8c13d08e3e",
   "metadata": {},
   "source": [
    "- **Setting a trigger**\n",
    "\n",
    "Setting a trigger can be done when setting the *WindowInto* transform for a PCollection. It requires also setting the *AccumulationMode* parameter which can take two alternatives: **ACCUMULATING** or **DISCARDING**\n",
    "\n",
    "When a trigger fires, it emits the current contents of the window as a pane. Therefore, when the *AccumulationMode* parameter is set to *ACCUMULATING*, data is collected across the window's panes as the trigger fires. However, when set to *DISCARDING*, the processed panes (data) of each firing triggers are mutually exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661d897-9b99-4882-844b-f7f17cdc781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  pcollection | WindowInto(\n",
    "    FixedWindows(1 * 60),\n",
    "    trigger=AfterProcessingTime(1 * 60),\n",
    "    accumulation_mode=AccumulationMode.DISCARDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d707c7a-bb66-4fa6-afc4-b3e52ccfac42",
   "metadata": {},
   "source": [
    "- **Composite triggers**\n",
    "\n",
    "Depending on your use case, you might want to use composite triggers that are made up of the ones we already describe.\n",
    "For more info : https://beam.apache.org/documentation/programming-guide/#composite-triggers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
